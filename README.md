# ğŸ¤– Natural Language Processing & AI Concepts  

Welcome to this **NLP & AI Concepts Repository** ğŸ¯  
This repo contains simplified explanations of **core concepts** used in **Machine Learning, Deep Learning, and Generative AI** ğŸš€  

---

## ğŸ“š Contents  
- ğŸ”¹ [LLM (Large Language Models)](#-llm-large-language-models)  
- ğŸ”¹ [NLTK](#-nltk-natural-language-toolkit)  
- ğŸ”¹ [spaCy](#-spacy)  
- ğŸ”¹ [Text Processing Types](#-text-processing-types)  
- ğŸ”¹ [N-Grams](#-n-grams)  
- ğŸ”¹ [Vectors](#-vectors)  
- ğŸ”¹ [Bag of Words (BoW)](#-bag-of-words-bow)  
- ğŸ”¹ [Embeddings](#-embeddings)  
- ğŸ”¹ [BERT](#-bert)  
- ğŸ”¹ [Transformers](#-transformers)  
- ğŸ”¹ [Prompt Engineering](#-prompt-engineering)  
- ğŸ”¹ [RAG (Retrieval-Augmented Generation)](#-rag-retrieval-augmented-generation)  
- ğŸ”¹ [Hugging Face](#-hugging-face)  
- ğŸ”¹ [MCP (Model Context Protocol)](#-mcp-model-context-protocol)  
- ğŸ”¹ [AI Agents](#-ai-agents)  
- ğŸ”¹ [FAISS](#-faiss)  
- ğŸ”¹ [Word Embeddings](#-word-embeddings)  

---

## ğŸ§  LLM (Large Language Models)  
**Definition:**  
LLMs are **deep learning models** trained on massive amounts of text data to understand, generate, and reason with human language.  
âœ… Examples: GPT, LLaMA, Gemini.  

---

## ğŸ“˜ NLTK (Natural Language Toolkit)  
- Python library for NLP.  
- Used for **tokenization, stemming, lemmatization, stopwords removal, POS tagging, parsing**.  
âœ… Example: `from nltk.tokenize import word_tokenize`  

---

## âš¡ spaCy  
- **Fast NLP library** optimized for production.  
- Features: Tokenization, POS tagging, Named Entity Recognition (NER), Dependency Parsing.  
âœ… Example: `import spacy; nlp = spacy.load("en_core_web_sm")`  

---

## âœ‚ï¸ Text Processing Types  
- **Tokenization** â€“ Split text into words/sentences.  
- **Stopword Removal** â€“ Remove common words (is, the, and).  
- **Stemming** â€“ Reduce words to root (playing â†’ play).  
- **Lemmatization** â€“ Context-aware root word (better â†’ good).  
- **POS Tagging** â€“ Identify nouns, verbs, adjectives.  

---

## ğŸ”¡ N-Grams  
An **N-Gram** is a continuous sequence of **N items (words, characters, or tokens)** from a text.  
They are widely used in **text analysis, language modeling, and NLP tasks**.  

--- 
- **Types:**  
  - Unigram: 1 word â†’ "AI"  
  - Bigram: 2 words â†’ "AI Agent"  
  - Trigram: 3 words â†’ "AI is powerful"  

---

## ğŸ§© Vectors  
- **Mathematical representation of words/sentences** in numerical form.  
- **Types:**  
  - One-hot Encoding  
  - TF-IDF  
  - Dense Embeddings  

## ğŸ§© Vectors  

In NLP, **vectors** are the numerical representation of text (words, sentences, or documents).  
They allow algorithms to **process and understand language mathematically**.  

---

### ğŸ“‘ Types of Vectors  

1. **ğŸ”¹ One-Hot Encoding**  
   - Each word is represented as a **binary vector**.  
   - The position of `1` indicates the word, and the rest are `0`s.  
   - âœ… Example (Vocabulary = ["AI", "is", "powerful"]):  
     - "AI" â†’ [1, 0, 0]  
     - "is" â†’ [0, 1, 0]  
     - "powerful" â†’ [0, 0, 1]  
   - âš ï¸ Limitation: High-dimensional & does not capture meaning.  

2. **ğŸ”¹ TF-IDF (Term Frequency â€“ Inverse Document Frequency)**  
   - Represents text based on **importance of words**.  
   - Formula:  
     - **TF** = How often a word appears in a document.  
     - **IDF** = How unique the word is across all documents.  
   - âœ… Example:  
     - Common word like "the" â†’ low score.  
     - Rare but important word like "neural" â†’ high score.  
   - âš ï¸ Still ignores context & word order.  

3. **ğŸ”¹ Dense Embeddings**  
   - Words/sentences represented as **low-dimensional dense vectors**.  
   - Captures **semantic meaning** and similarity.  
   - Generated using **neural networks** (Word2Vec, GloVe, FastText, BERT, etc.).  
   - âœ… Example:  
     - "king" and "queen" will be **close** in vector space.  
     - "king" and "car" will be **far apart**.  
   - âš¡ Advantage: Captures context, meaning, and relationships.  

---

## ğŸ“¦ Bag of Words (BoW)

**Definition:**  
Bag of Words (BoW) is a simple way to represent text as a **numerical vector** based on **word frequency**.  
It **ignores grammar, word order, and context**.  

---

### ğŸ”¹ How it Works
1. Identify all **unique words** in the text (vocabulary).  
2. Count **how many times each word appears**.  
3. Represent the text as a **vector or dictionary** with these counts.  

---

# ğŸ”‘ Embeddings  

Embeddings are **dense vector representations of text** that capture **semantic meaning**.  
Unlike Bag-of-Words or TF-IDF (which are sparse), embeddings place similar words closer in a **vector space**.  


### ğŸ“‘ Types of Embeddings  

1. **ğŸ”¹ Word2Vec** (Google, 2013)  
   - Learns word embeddings using **neural networks**.  
   - Two architectures:  
     - **CBOW (Continuous Bag of Words):** Predicts a word from its context.  
     - **Skip-Gram:** Predicts surrounding context words from a given word.  
   - âœ… Example: `"king - man + woman â‰ˆ queen"`  
   - Usage: Semantic similarity, text clustering.  

2. **ğŸ”¹ GloVe (Global Vectors for Word Representation)**  
   - Developed by **Stanford**.  
   - Uses **word co-occurrence matrix** + statistical information.  
   - Captures both **global statistics** (entire corpus) and **local context**.  
   - âœ… Example: Better at analogies and global word relationships.  
   - Usage: NLP tasks where global corpus meaning is important.  

3. **ğŸ”¹ FastText (by Facebook/Meta)**  
   - Extension of Word2Vec.  
   - Breaks words into **character n-grams** â†’ better for **rare & OOV words**.  
   - âœ… Example: `"playing"` â†’ subwords: `"play"`, `"lay"`, `"ing"`.  
   - Usage: Morphologically rich languages (e.g., German, Finnish, Tamil).  

4. **ğŸ”¹ Transformer-based Embeddings**
   - Transformers are a **deep learning architecture** based on the **Attention mechanism** âš¡.  
   - Unlike RNNs/LSTMs (which process sequentially), Transformers process input **in parallel** â†’ much faster and more accurate for long text.  
   - Derived from **Transformer models** (BERT, RoBERTa, OpenAI, etc.).  
   - Contextual â†’ meaning depends on **surrounding words**.  
   - âœ… Example:  
     - "bank" (river bank ğŸŒŠ) vs "bank" (financial ğŸ¦) â†’ different embeddings.  
   - Usage: State-of-the-art NLP tasks (QA, summarization, semantic search).  

---
## ğŸ BERT  
- **Bidirectional Encoder Representations from Transformers**.  
- Reads text **both left-to-right & right-to-left**.  
- Used for **text classification, sentiment analysis, QA systems**.  

---

## ğŸ“ Prompt Engineering  
- Crafting **effective prompts** for LLMs.  
- **Types:**  
  - Zero-shot â†’ No example  
  - Few-shot â†’ With examples  
  - Chain-of-thought â†’ Step-by-step reasoning  
  - Instruction-tuning â†’ Direct command style  

---

## ğŸ“¡ RAG (Retrieval-Augmented Generation)  
- Combines **LLMs + external knowledge base**.  
- **Techniques:**  
  - Vector search (using FAISS)  
  - Retrieval pipelines  
  - Hybrid RAG (structured + unstructured data)  

---

## ğŸ¤— Hugging Face  
- Open-source hub for **NLP models, datasets, transformers**.  
- Provides `transformers` library for easy model use.  

---

## ğŸ”— MCP (Model Context Protocol)  

**MCP (Model Context Protocol)** is a **standardized framework** designed to connect **AI models (LLMs) with external tools, APIs, and data sources**.  
It ensures that models can **understand, retrieve, and interact with external information** in a consistent way.  

---

### ğŸ“‘ Key Features of MCP  

1. **Interoperability**  
   - Allows different AI models and systems to **communicate seamlessly**.  
   - Example: Connecting a GPT model with a database or a search engine.  

2. **Context Management**  
   - Provides a **structured context** for models to make decisions.  
   - Example: Passing user data, previous conversations, or external API responses to the model.  

3. **Tool & API Integration**  
   - Enables models to **invoke tools** or external services reliably.  
   - Example: Calling a weather API or retrieving a document from a knowledge base.  

4. **Standardized Communication**  
   - Defines **protocols** for how models exchange messages with external systems.  
   - Reduces **errors** and improves **scalability**.  

---

### ğŸ”¹ Example Use Case  

- **Scenario:** A chatbot needs to answer user queries about stock prices.  
  1. User asks: *"What is Tesla's stock price today?"*  
  2. GPT model receives the prompt.  
  3. MCP allows GPT to **query a stock API** and retrieve real-time data.  
  4. Model responds with **accurate, up-to-date information**.  

---

## ğŸ¤– AI Agents  
- Autonomous systems powered by LLMs.  
- Can **plan, reason, and act** using tools (APIs, databases).  
âœ… Example: LangChain, AutoGPT, BabyAGI.  

---

## ğŸ—‚ï¸ FAISS (Facebook AI Similarity Search)  
- Library for **fast similarity search** in large vector datasets.  
- Used in **semantic search, RAG, recommendation systems**.  

---

## ğŸš€ Interactive Demo Links  
- [ğŸ”— Hugging Face Models](https://huggingface.co/models)  
- [ğŸ”— TensorFlow Playground](https://playground.tensorflow.org/)  
- [ğŸ”— NLP Visualization Tools](https://explosion.ai/demos)  

---
